#!/usr/bin/env python

import os
import os.path as osp
import json
import click
import torch
from skimage import io
from tqdm import tqdm
from bvpr.data.corpus import Corpus
from bvpr.data.colorization import COLOR_WORDS, PAD_TOKEN, UNK_TOKEN, SOS_TOKEN
from torchtext.data import get_tokenizer


def count_words(data):
    count_dict = dict()
    tokenizer = get_tokenizer("basic_english")
    for entry in data:
        caption = entry["caption"].lower()
        tokens = tokenizer(caption)
        for token in tokens:
            count_dict[token] = 1 + count_dict.get(token, 0)
    return count_dict


def create_vocab(output_dir, year=2017, min_occur=0):
    print("creating vocab for train{}, min_occur={}".format(year, min_occur))
    corpus_filename = f"train{year}-corpus-{min_occur}.pth"
    corpus_file = osp.join(output_dir, corpus_filename)
    captions_file = osp.join(output_dir, f"captions_train{year}.json")
    with open(osp.join(output_dir, captions_file)) as f:
        captions = json.load(f)
    count_dict = count_words(captions["annotations"])
    word_counts = sorted(count_dict.items(), key=lambda x: x[1], reverse=True)
    word_counts = [
        (word, count)
        for (word, count) in word_counts
        if count >= min_occur]
    corpus = Corpus()
    corpus.dictionary.add_word(PAD_TOKEN)
    for (word, _) in word_counts:
        corpus.dictionary.add_word(word)
    corpus.dictionary.add_word(UNK_TOKEN)
    corpus.dictionary.add_word(SOS_TOKEN)
    torch.save(corpus, corpus_file)


def create_split(input_dir, output_dir, split="train", year=2017):
    print("split={}{}".format(split, year))
    instances_filename = f"instances_{split}{year}.json"
    captions_filename = f"captions_{split}{year}.json"
    tokenizer = get_tokenizer("basic_english")

    print("load captions...")
    with open(osp.join(input_dir, captions_filename)) as f:
        captions = json.load(f)

    print("load instances...")
    with open(osp.join(input_dir, instances_filename)) as f:
        instances = json.load(f)

    print("process captions...")
    c_annotations = list()
    image_ids = set()
    for entry in tqdm(captions["annotations"]):
        tokens = tokenizer(entry["caption"].lower())
        for t in tokens:
            if t in COLOR_WORDS:
                c_annotations.append(entry)
                image_ids.add(entry["image_id"])
                break

    c_images = list()
    for entry in tqdm(captions["images"]):
        if entry["id"] in image_ids:
            c_images.append(entry)

    print("process instances...")
    i_annotations, i_images = list(), list()
    for entry in tqdm(instances["images"]):
        if entry["id"] in image_ids:
            i_images.append(entry)

    for entry in tqdm(instances["annotations"]):
        if entry["image_id"] in image_ids:
            i_annotations.append(entry)

    captions["images"] = c_images
    captions["annotations"] = c_annotations
    instances["images"] = i_images
    instances["annotations"] = i_annotations

    captions["info"]["description"] = "COCO 2017 Dataset for Colorization"
    captions["info"]["date_created"] = "2021/09/27"
    instances["info"] = captions["info"]

    print("save captions...")
    with open(osp.join(output_dir, captions_filename), "w") as f:
        json.dump(captions, f)

    print("save instances...")
    with open(osp.join(output_dir, instances_filename), "w") as f:
        json.dump(instances, f)


@click.command()
@click.option("--input-dir", required=True, type=click.Path(exists=True))
@click.option("--output-dir", required=True, type=click.Path())
def main(input_dir, output_dir):
    input_dir = osp.abspath(osp.expanduser(input_dir))
    output_dir = osp.abspath(osp.expanduser(output_dir))
    if not osp.isdir(output_dir):
        os.makedirs(output_dir)

    create_split(input_dir, output_dir)
    create_split(input_dir, output_dir, year=2014)
    create_split(input_dir, output_dir, "val")
    create_split(input_dir, output_dir, split="val", year=2014)

    create_vocab(output_dir)
    create_vocab(output_dir, min_occur=5)
    create_vocab(output_dir, year=2014)
    create_vocab(output_dir, year=2014, min_occur=5)


if __name__ == "__main__":
    main()